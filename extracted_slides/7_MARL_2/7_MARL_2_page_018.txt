Centralized Training with Decentralized Execution

• Each agent has its own policy network (actor): $\pi(a^i \vert o^i; \boldsymbol{\theta}^i)$.

• The central controller has $n$ value networks (critics): $q(\mathbf{o}, \mathbf{a}; \mathbf{w}^i)$.

• Centralized Training: During training, the central controller knows all the agents' observations, actions, and rewards.

• Decentralized Execution: During execution, the central controller and its value networks are not used.

Reference:

1. Lowe et al. Multi-agent actor-critic for mixed cooperative-competitive environments. In NIPS, 2017.
2. Foerster et al. Counterfactual multi-agent policy gradients. In AAAI, 2018.