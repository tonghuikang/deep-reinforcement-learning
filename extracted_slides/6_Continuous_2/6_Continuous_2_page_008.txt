Updating Value Network by TD

• Transition: $(s_t, a_t, r_t, s_{t+1})$.

state s → Policy Network (Parameter: θ) → action a = π(s; θ) → Value Network (Parameter: w) → value q(s,a; w)