Policy Network

Approximate policy function, $\pi(a \vert s)$, by policy network, $\pi(a \vert s; \boldsymbol{\theta})$.

[THIS IS FIGURE: A diagram showing a Mario game scene as input "state s", connected through "Conv" (convolutional layer), "Dense" layer, and "Softmax" layer to output probabilities: "left", 0.2; "right", 0.1; "up", 0.7. The layers are connected with arrows showing "feature" extraction.]