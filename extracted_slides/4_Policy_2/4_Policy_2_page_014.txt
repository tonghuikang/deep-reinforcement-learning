Approximations

Stochastic policy gradient:
$$\boldsymbol{g}(a_t) = \frac{\partial \ln \pi(a_t \vert s_t; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot \left(Q_\pi(s_t, a_t) - V_\pi(s_t)\right).$$

â€¢ Approximate $V(s; \boldsymbol{\theta})$ by the value network, $v(s; \boldsymbol{w})$.