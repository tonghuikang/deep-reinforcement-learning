Algorithm

1. Observe the state $s_t$.

2. Randomly sample action $a_t$ according to $\pi(\cdot \vert s_t; \boldsymbol{\theta}_t)$.

3. Compute $q_t \approx Q_\pi(s_t, a_t)$ (some estimate).

4. Differentiate policy network: $\mathbf{d}_{g,t} = \frac{\partial \log \pi(a_t \vert s_t,\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \vert_{\boldsymbol{\theta}=\boldsymbol{\theta}_t}$.

5. (Approximate) policy gradient: $\mathbf{g}(a_t, \boldsymbol{\theta}_t) = q_t \cdot \mathbf{d}_{g,t}$.

6. Update policy network: $\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t + \beta \cdot \mathbf{g}(a_t, \boldsymbol{\theta}_t)$.