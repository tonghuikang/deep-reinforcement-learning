Train DQN using TD learning

• Prediction: $Q(s_t, a_t; \mathbf{w}_t)$.

• TD target:
    $y_t = r_t + \gamma \cdot Q(s_{t+1}, a_{t+1}; \mathbf{w}_t)$
    $= r_t + \gamma \cdot \max_a Q(s_{t+1}, a; \mathbf{w}_t)$.

• Loss: $L_t = \frac{1}{2}[Q(s_t, a_t; \mathbf{w}) - y_t]^2$.

• Gradient descent: $\mathbf{w}_{t+1} = \mathbf{w}_t - \alpha \cdot \frac{\partial L_t}{\partial \mathbf{w}} \big\vert_{\mathbf{w}=\mathbf{w}_t}$.