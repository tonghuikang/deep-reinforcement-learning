How to apply TD learning to DQN?

Identity: $U_t = R_t + \gamma \cdot U_{t+1}$.

TD learning for DQN:

• DQN's output, $Q(s_t, a_t; \mathbf{w})$, is an estimate of $U_t$.

• DQN's output, $Q(s_{t+1}, a_{t+1}; \mathbf{w})$, is an estimate of $U_{t+1}$.

• Thus,    $Q(s_t, a_t; \mathbf{w}) \approx \mathbb{E}[R_t + \gamma \cdot Q(s_{t+1}, A_{t+1}; \mathbf{w})]$.
           
           estimate of $U_t$        estimate of $U_{t+1}$