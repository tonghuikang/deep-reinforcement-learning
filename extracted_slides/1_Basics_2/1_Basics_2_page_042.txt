How to apply TD learning to DQN?

Identity: $U_t = R_t + \gamma \cdot U_{t+1}$.

TD learning for DQN:

• DQN's output, $Q(s_t, a_t; \mathbf{w})$, is an estimate of $U_t$.

• DQN's output, $Q(s_{t+1}, a_{t+1}; \mathbf{w})$, is an estimate of $U_{t+1}$.

• Thus,    $Q(s_t, a_t; \mathbf{w}) \approx r_t + \gamma \cdot Q(s_{t+1}, a_{t+1}; \mathbf{w})$.

           Prediction                TD target