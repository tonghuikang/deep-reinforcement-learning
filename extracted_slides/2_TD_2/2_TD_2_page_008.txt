Derive TD Target

• We have proved that for all $\pi$,

$$Q_\pi(s_t, a_t) = \mathbb{E}[R_t + \gamma \cdot Q_\pi(S_{t+1}, A_{t+1})]$$.

• If $\pi$ is the optimal policy $\pi^*$, then

$$Q_{\pi^*}(s_t, a_t) = \mathbb{E}[R_t + \gamma \cdot Q_{\pi^*}(S_{t+1}, A_{t+1})]$$.

• $Q_{\pi^*}$ and $Q^*$ both denote the optimal action-value function.

Identity: $Q^*(s_t, a_t) = \mathbb{E}[R_t + \gamma \cdot Q^*(S_{t+1}, A_{t+1})]$.