Function Approximation to State-Value

Approximate stochastic policy gradient:

g(a_t) \approx \frac{\partial \ln \pi(a_t \vert s_t; \theta)}{\partial \theta} \cdot (r_t + \gamma \cdot V_\pi(s_{t+1}) - V_\pi(s_t))

â€¢ Approximate V_\pi(s) by the value network v(s; \mathbf{w})