MC Approximation to Action-Value

Stochastic policy gradient:

g(a_t) = \frac{\partial \ln \pi(a_t \vert s_t; \theta)}{\partial \theta} \cdot (Q_\pi(s_t, a_t) - V_\pi(s_t))

\approx r_t + \gamma \cdot V_\pi(s_{t+1})