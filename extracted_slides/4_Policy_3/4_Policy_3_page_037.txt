Updating Policy Network

Approximate stochastic policy gradient:

g(a_t) \approx \frac{\partial \ln \pi(a_t \vert s_t; \theta)}{\partial \theta} \cdot (r_t + \gamma \cdot v(s_{t+1}; \mathbf{w}) - v(s_t; \mathbf{w}))

Denote it by y_t

Policy gradient ascent:

\theta \leftarrow \theta + \beta \cdot \frac{\partial \ln \pi(a_t \vert s_t; \theta)}{\partial \theta} \cdot (y_t - v(s_t; \mathbf{w}))